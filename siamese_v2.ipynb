{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T11:30:28.275103",
     "start_time": "2017-10-25T11:30:28.157818"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import classification_report\n",
    "from itertools import product\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "input_dim = 784\n",
    "num_classses = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T09:53:49.947713",
     "start_time": "2017-10-25T09:53:49.414933"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X, y):\n",
    "    #create tensor variant of 2D images\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        X = X.reshape(X.shape[0], 1, img_rows, img_cols)\n",
    "        input_shape = (1, img_rows, img_cols)\n",
    "    else:\n",
    "        X = X.reshape(X.shape[0], img_rows, img_cols, 1)\n",
    "        input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "    X = X.astype('float32') / 255\n",
    "    #X = (X - np.mean(X)) / np.std(X)\n",
    "    # convert class vectors to binary class matrices\n",
    "    #y = keras.utils.to_categorical(y, num_classes)\n",
    "    return X, y, input_shape\n",
    "\n",
    "data=np.load(\"FashionData/FashionPDEngDM.npz\")\n",
    "\n",
    "## Labeled training set for classes 1,2,3,8,9 (30000 samples)\n",
    "x_train_12389_labeled = data[\"x_train_12389_labeled\"]\n",
    "y_train_12389_labeled = data[\"y_train_12389_labeled\"]\n",
    "\n",
    "## Labeled training set for classes 0,4,5,6,7 (just 5 samples)\n",
    "x_train_04567_labeled=data[\"x_train_04567_labeled\"]\n",
    "y_train_04567_labeled=data[\"y_train_04567_labeled\"]\n",
    "\n",
    "## Unlabeled training set for classes 0,4,5,6,7 (29992 samples)>\n",
    "x_train_04567_unlabeled=data[\"x_train_04567_unlabeled\"]\n",
    "\n",
    "## Labeled test set for classes 1,2,3,8,9\n",
    "x_test_12389=data[\"x_test_12389\"]\n",
    "y_test_12389=data[\"y_test_12389\"]\n",
    "\n",
    "##Labeled test set for classes 0,4,5,6,7 (this is where we are interested to obtain the highest accuracy possible - project goal)\n",
    "x_test_04567=data[\"x_test_04567\"]\n",
    "y_test_04567=data[\"y_test_04567\"]\n",
    "\n",
    "x_train_12389_labeled, y_train_12389_labeled, input_shape = preprocess_data(x_train_12389_labeled, y_train_12389_labeled)\n",
    "x_test_12389, y_test_12389, _ = preprocess_data(x_test_12389, y_test_12389)\n",
    "\n",
    "x_train_04567_labeled, y_train_04567_labeled, input_shape = preprocess_data(x_train_04567_labeled, y_train_04567_labeled)\n",
    "x_test_04567, y_test_04567, _ = preprocess_data(x_test_04567, y_test_04567)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T10:37:59.058552",
     "start_time": "2017-10-25T10:37:59.051535"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# image augumentation\n",
    "rotation_range = 50\n",
    "width_shift_range = 0.5\n",
    "height_shift_range = 0.5\n",
    "shear_range = 0.5\n",
    "horizontal_flip = True\n",
    "vertical_flip = True\n",
    "fill_mode = 'nearest'\n",
    "zoom_range = 0.2\n",
    "x_train_04567_aug = ImageDataGenerator(\n",
    "    rotation_range = rotation_range,\n",
    "    shear_range = shear_range,\n",
    "    width_shift_range= width_shift_range,\n",
    "    height_shift_range= height_shift_range,\n",
    "    horizontal_flip = horizontal_flip,\n",
    "    vertical_flip = vertical_flip,\n",
    "    zoom_range = zoom_range,\n",
    "    fill_mode = fill_mode\n",
    ")\n",
    "x_train_04567_aug.fit(x_train_04567_labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T11:31:04.215182",
     "start_time": "2017-10-25T11:31:01.637388"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# display and generate augmented data \n",
    "def get_augmented_data(generator, x, y, N, visualize = False):\n",
    "    \"\"\"Generate augmented data.\n",
    "    \n",
    "    Params: \n",
    "    - generator: ImageGenerator \n",
    "    - x: X data\n",
    "    - y: y data\n",
    "    - N: number of images to generates per class\n",
    "    - visualize: whether to visualize the augmented images\n",
    "    \n",
    "    Return: (x, y) augmneted data\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train_04567_aug_generated = []\n",
    "    y_train_04567_aug_generated = []\n",
    "\n",
    "    data_len = len(x)    \n",
    "    data_gen = generator.flow(x, y, batch_size = data_len)\n",
    "    num_batch_print = 2\n",
    "    num_columns = 2\n",
    "\n",
    "    for i, data_batch in enumerate(data_gen):    \n",
    "        if N <= i: # in every iteration, one instance per class is generated\n",
    "            break\n",
    "       \n",
    "        x_batch, y_batch = data_batch\n",
    "        #save data\n",
    "        x_train_04567_aug_generated.append(x_batch)\n",
    "        y_train_04567_aug_generated.append(y_batch)\n",
    "        \n",
    "        if visualize: # visualize the augmented results\n",
    "            f, axarr = plt.subplots(data_len, num_columns, figsize = (5, 5))\n",
    "            for j in range(len(x_batch)):\n",
    "                ground_ind = np.where(y == y_batch[j])\n",
    "                axarr[j, 0].imshow(x[ground_ind].reshape((28,28)), cmap='gray')   \n",
    "                axarr[j, 1].imshow(x_batch[j].reshape((28,28)), cmap='gray')\n",
    "                axarr[j, 0].axis('off')\n",
    "                axarr[j, 1].axis('off')\n",
    "            print(i)\n",
    "            plt.show()\n",
    "    \n",
    "    x_train_04567_aug_generated[-1]\n",
    "    # reshaping such that the array has the shape of (# data, 28, 28, 1)\n",
    "    x_train_04567_aug_generated = np.array(x_train_04567_aug_generated).reshape((-1, 28, 28, 1)) \n",
    "  \n",
    "    #reshaping to shape of (# data)\n",
    "    y_train_04567_aug_generated = np.array(y_train_04567_aug_generated).reshape((-1)) \n",
    "#    modulo = N % data_len\n",
    "#     if modulo != 0:\n",
    "#         x_train_04567_aug_generated = x_train_04567_aug_generated[0 : -(data_len-modulo)]\n",
    "#         y_train_04567_aug_generated = y_train_04567_aug_generated[0 : -(data_len-modulo)]\n",
    "#         print(\"Readjusting the shape. The shape is %s \" % (x_train_04567_aug_generated.shape, ))\n",
    "        #print(\"Warning: The data lenght will be slightly bigger than N, because N is not divadable by size of data.\")\n",
    "    return x_train_04567_aug_generated, y_train_04567_aug_generated\n",
    "\n",
    "x_train_04567_aug_generated, y_train_04567_aug_generated = get_augmented_data(generator = x_train_04567_aug, \n",
    "                                                                              x = x_train_04567_labeled,\n",
    "                                                                              y = y_train_04567_labeled,\n",
    "                                                                              N = 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T11:31:05.022316",
     "start_time": "2017-10-25T11:31:04.216142"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training pairs is 20993 \n",
      "Number of val pairs is 8997 \n"
     ]
    }
   ],
   "source": [
    "# create training+test positive and negative pairs\n",
    "def create_pairs(x, class_indices, num_classes):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs. \n",
    "\n",
    "    Maximally creates (min(class_size) - 1) * num_classes pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []    \n",
    "    n = min([len(class_indices[d]) for d in range(num_classes)]) - 1\n",
    "    for d in range(num_classes):\n",
    "        for i in range(n):\n",
    "            z1, z2 = class_indices[d][i], class_indices[d][i+1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1, num_classes)\n",
    "            dn = (d + inc) % num_classes # this guarantees that the same class will not be selected\n",
    "            z1, z2 = class_indices[d][i], class_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "ind_04567 = [0,4,5,6,7]\n",
    "class_indices = [np.where(y_train_04567_aug_generated == i)[0] for i in ind_04567]\n",
    "x_pairs, y_pairs = create_pairs(x_train_04567_aug_generated, class_indices, int(num_classes / 2))\n",
    "x_train_pairs, x_val_pairs, y_train_pairs, y_val_pairs = train_test_split(x_pairs, y_pairs, test_size=0.3, random_state=42)\n",
    "print(\"Number of training pairs is %d \" % x_train_pairs.shape[0])\n",
    "print(\"Number of val pairs is %d \" % x_val_pairs.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T12:49:29.606492",
     "start_time": "2017-10-25T12:49:29.338780"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azika\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:38: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"de..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "# model definition\n",
    "def create_base_network(input_shape):\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    #model.add(Dense(128, activation='relu'))\n",
    "    #model.add(Dropout(0.5))\n",
    "    return model\n",
    "\n",
    "def get_abs_diff(vects):\n",
    "    x, y = vects\n",
    "    return K.abs(x - y)  \n",
    "\n",
    "def abs_diff_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return shape1  \n",
    "\n",
    "base_network = create_base_network(input_shape)\n",
    "\n",
    "input_a = Input(shape = input_shape)\n",
    "input_b = Input(shape = input_shape)\n",
    "\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network will be shared across the two branches\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "abs_diff = Lambda(get_abs_diff, output_shape = abs_diff_output_shape)([processed_a, processed_b])\n",
    "\n",
    "flattened_weighted_distance = Dense(1, activation = 'sigmoid')(abs_diff)\n",
    "\n",
    "siamese_model = Model(input=[input_a, input_b], output = flattened_weighted_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T13:58:43.120769",
     "start_time": "2017-10-25T12:49:30.275187"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\azika\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:22: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "20993/20993 [==============================] - 80s - loss: 0.6934 - acc: 0.5159    \n",
      "Epoch 2/50\n",
      "20993/20993 [==============================] - 73s - loss: 0.6888 - acc: 0.5380    \n",
      "Epoch 3/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6692 - acc: 0.5875    \n",
      "Epoch 4/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6536 - acc: 0.6127    \n",
      "Epoch 5/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6400 - acc: 0.6270    \n",
      "Epoch 6/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6342 - acc: 0.6337    \n",
      "Epoch 7/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6318 - acc: 0.6360    \n",
      "Epoch 8/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.6170 - acc: 0.6496    \n",
      "Epoch 9/50\n",
      "20993/20993 [==============================] - 75s - loss: 0.6281 - acc: 0.6423    \n",
      "Epoch 10/50\n",
      "20993/20993 [==============================] - 74s - loss: 0.6029 - acc: 0.6654    \n",
      "Epoch 11/50\n",
      "20993/20993 [==============================] - 71s - loss: 0.6004 - acc: 0.6684    \n",
      "Epoch 12/50\n",
      "20993/20993 [==============================] - 73s - loss: 0.6009 - acc: 0.6662    \n",
      "Epoch 13/50\n",
      "20993/20993 [==============================] - 73s - loss: 0.5881 - acc: 0.6805    \n",
      "Epoch 14/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.5852 - acc: 0.6809    \n",
      "Epoch 15/50\n",
      "20993/20993 [==============================] - 73s - loss: 0.5764 - acc: 0.6926    \n",
      "Epoch 16/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.5824 - acc: 0.6867    \n",
      "Epoch 17/50\n",
      "20993/20993 [==============================] - 72s - loss: 0.5729 - acc: 0.6897    \n",
      "Epoch 18/50\n",
      "20993/20993 [==============================] - 75s - loss: 0.5732 - acc: 0.6908    \n",
      "Epoch 19/50\n",
      "20993/20993 [==============================] - 77s - loss: 0.5972 - acc: 0.6738    \n",
      "Epoch 20/50\n",
      "20993/20993 [==============================] - 78s - loss: 0.5725 - acc: 0.6918    \n",
      "Epoch 21/50\n",
      "20993/20993 [==============================] - 78s - loss: 0.5689 - acc: 0.6945    \n",
      "Epoch 22/50\n",
      "20993/20993 [==============================] - 79s - loss: 0.5719 - acc: 0.6896    \n",
      "Epoch 23/50\n",
      "20993/20993 [==============================] - 178s - loss: 0.5660 - acc: 0.6981   \n",
      "Epoch 24/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5570 - acc: 0.7075    \n",
      "Epoch 25/50\n",
      "20993/20993 [==============================] - 84s - loss: 0.5577 - acc: 0.7046    \n",
      "Epoch 26/50\n",
      "20993/20993 [==============================] - 84s - loss: 0.5543 - acc: 0.7102    \n",
      "Epoch 27/50\n",
      "20993/20993 [==============================] - 85s - loss: 0.5537 - acc: 0.7058    \n",
      "Epoch 28/50\n",
      "20993/20993 [==============================] - 84s - loss: 0.5452 - acc: 0.7158    \n",
      "Epoch 29/50\n",
      "20993/20993 [==============================] - 87s - loss: 0.5452 - acc: 0.7130    \n",
      "Epoch 30/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5461 - acc: 0.7109    \n",
      "Epoch 31/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5516 - acc: 0.7100    \n",
      "Epoch 32/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5439 - acc: 0.7153    \n",
      "Epoch 33/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5423 - acc: 0.7149    \n",
      "Epoch 34/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5357 - acc: 0.7203    \n",
      "Epoch 35/50\n",
      "20993/20993 [==============================] - 89s - loss: 0.5434 - acc: 0.7150    \n",
      "Epoch 36/50\n",
      "20993/20993 [==============================] - 91s - loss: 0.5432 - acc: 0.7155    \n",
      "Epoch 37/50\n",
      "20993/20993 [==============================] - 85s - loss: 0.5359 - acc: 0.7204    \n",
      "Epoch 38/50\n",
      "20993/20993 [==============================] - 87s - loss: 0.5454 - acc: 0.7143    \n",
      "Epoch 39/50\n",
      "20993/20993 [==============================] - 88s - loss: 0.5417 - acc: 0.7140    \n",
      "Epoch 40/50\n",
      "20993/20993 [==============================] - 83s - loss: 0.5319 - acc: 0.7266    \n",
      "Epoch 41/50\n",
      "20993/20993 [==============================] - 83s - loss: 0.5358 - acc: 0.7212    \n",
      "Epoch 42/50\n",
      "20993/20993 [==============================] - 83s - loss: 0.5314 - acc: 0.7254    \n",
      "Epoch 43/50\n",
      "20993/20993 [==============================] - 82s - loss: 0.5298 - acc: 0.7241    \n",
      "Epoch 44/50\n",
      "20993/20993 [==============================] - 82s - loss: 0.5661 - acc: 0.6994    \n",
      "Epoch 45/50\n",
      "20993/20993 [==============================] - 82s - loss: 0.5335 - acc: 0.7229    \n",
      "Epoch 46/50\n",
      "20993/20993 [==============================] - 86s - loss: 0.5272 - acc: 0.7245    \n",
      "Epoch 47/50\n",
      "20993/20993 [==============================] - 96s - loss: 0.5248 - acc: 0.7298    \n",
      "Epoch 48/50\n",
      "20993/20993 [==============================] - 86s - loss: 0.5291 - acc: 0.7239    \n",
      "Epoch 49/50\n",
      "20993/20993 [==============================] - 85s - loss: 0.5296 - acc: 0.7261    \n",
      "Epoch 50/50\n",
      "20993/20993 [==============================] - 84s - loss: 0.5249 - acc: 0.7269    \n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "def step_decay(epoch):\n",
    "    '''Learning rate step decay following the original paper.'''\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.99\n",
    "    epochs_drop = 1\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "\n",
    "nb_epoch = 50\n",
    "optimizer = Adam()\n",
    "siamese_model.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [lrate]\n",
    "\n",
    "siamese_model.fit([x_train_pairs[:, 0], x_train_pairs[:, 1]], #pairs \n",
    "          y_train_pairs, #labels of the pairs\n",
    "          callbacks = callbacks_list,\n",
    "          batch_size=128,\n",
    "          nb_epoch=nb_epoch)\n",
    "siamese_model.save('siamese_model_without_dense.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T13:58:53.946100",
     "start_time": "2017-10-25T13:58:43.121740"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# validation accuracy\n",
    "y_val_pred_proba = siamese_model.predict([x_val_pairs[:, 0], x_val_pairs[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T13:58:53.951112",
     "start_time": "2017-10-25T13:58:53.947101"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy of classifying the val set: 69.46%\n"
     ]
    }
   ],
   "source": [
    "# validation accuracy\n",
    "y_val_pred_class = y_val_pred_proba > 0.5 \n",
    "y_val_pred_class = y_val_pred_class.reshape(-1)\n",
    "val_accur = np.sum(y_val_pred_class == y_val_pairs) / len(y_val_pairs)\n",
    "print('* Accuracy of classifying the val set: {:.2%}'.format(val_accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T13:58:53.967155",
     "start_time": "2017-10-25T13:58:53.952115"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# generate examples (k per class) that will be couples with test data\n",
    "k = 5 # k from from analogy of k-means\n",
    "x_test_04567_aug_generated, y_test_04567_aug_generated = get_augmented_data(generator = x_train_04567_aug, \n",
    "                                                                            x = x_train_04567_labeled,\n",
    "                                                                            y = y_train_04567_labeled,\n",
    "                                                                            N = k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:01:21.061132",
     "start_time": "2017-10-25T13:58:53.968158"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# create pairs\n",
    "x_test_pairs = np.array(list(product(x_test_04567, x_test_04567_aug_generated)))\n",
    "\n",
    "# predict the probability of a pair being similar. \n",
    "y_test_pred_prob = siamese_model.predict([x_test_pairs[:, 0], x_test_pairs[:, 1]])\n",
    "\n",
    "# reshaping such that each row contains k*number of class probabilities \n",
    "y_test_pred_prob = y_test_pred_prob.reshape(len(x_test_04567), len(x_test_04567_aug_generated))\n",
    "\n",
    "# getting top k prediction for a class\n",
    "y_test_pred_class_inds = y_test_pred_prob.argsort(axis = 1)[:, -k:]\n",
    "\n",
    "# like argsort but only for the highest score\n",
    "#y_test_pred_class_inds = np.argmax(y_test_pred_prob, axis = 1)\n",
    "\n",
    "# reverse order -> the highest prob is on the first place (This does not work! it reserve the data )\n",
    "#y_test_pred_class_inds = y_test_pred_class_inds[::-1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:01:21.443186",
     "start_time": "2017-10-25T14:01:21.062132"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# generating class prediction\n",
    "y_test_pred_k = np.array([mode(y_test_04567_aug_generated[y_test_pred_class_inds[i]]) \n",
    "                                for i in range(len(y_test_pred_class_inds))])[:, 0]\n",
    "y_test_pred_k = y_test_pred_k.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:01:21.451579",
     "start_time": "2017-10-25T14:01:21.444560"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 4, 5, 6, 7]), array([1051, 1256,  508, 1837,  348], dtype=int64))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many prediction per class\n",
    "np.unique(y_test_pred_k, return_counts= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-25T14:01:21.459599",
     "start_time": "2017-10-25T14:01:21.453582"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Accuracy of classifying the test set: 29.12%\n"
     ]
    }
   ],
   "source": [
    "# final accuracy\n",
    "test_accur = np.sum(y_test_pred_k == y_test_04567) / len(y_test_04567)\n",
    "print('* Accuracy of classifying the test set: {:.2%}'.format(test_accur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
